---
title: "Data Validation"
author: "Rohan Krishnan"
date: "2025-08-09"
output: github_document
---

This notebook serves as the initial data validation and sanity check for the raw train.csv and test.csv datasets. Our goal is to verify the structural integrity of the data by examining dimensions, column types, and checking for missing and unique values before moving on to exploratory analysis and feature engineering.

## Import libraries

In this section, we load the core R packages required for our data validation. In this notebook, we'll only need tidyverse for data manipulation and display and our custom data validation functions from `R/data-validation.R`.

```{r, message=FALSE}
library(tidyverse)
source(here::here("R","data-validation.R"))
```

## Load train and test CSVs

We load the raw training and testing datasets from the data/raw directory using the `here` package to ensure a reproducible file path, regardless of where the project is run.


```{r}
# Use the here library to get relative paths from .Rproj directory
train_raw <- read.csv(here::here("data", "raw", "train.csv"))
test_raw <- read.csv(here::here("data", "raw", "test.csv"))
```

## Get a basic snapshot of what is contained in train.csv and test.csv

The first thing we look at is just the first 5 rows of the train and test CSV files.

```{r}
train_raw %>% head()
```
```{r}
test_raw %>% head()
```
We can see from the first 5 rows of both datasets that each row represents a "track" (or song) and its relevant features like duration in milliseconds, whether or not it is explicit, its danceablilty (score created by Spotify), and other similar features. An important difference to note between train.csv and test.csv is that test.csv does have the "popularity" (target) colummn. Since our task is to understand what makes a song popular and explore if it is possible to predict a song's popularity from its features, the current structure of the data is already ideal and we likely will not need to perform any aggregations.

## Look at the basic dataset structure for train.csv and test.csv

We perform a high-level comparison of the training and testing sets, examining their dimensions and identifying the target column, to ensure the datasets are correctly structured for our analysis.

```{r}
compare_train_test_raw_dims(train_df_raw = train_raw, test_df_raw = test_raw)
```
## Look at column data types for each dataset and make sure they match

Here we confirm that the data types of all matching columns are consistent between the training and testing sets. This is a critical check to ensure a smooth preprocessing pipeline.

```{r}
check_col_dtypes(train_df_raw = train_raw, test_df_raw = test_raw)
```

Since all of the matching columns have the same data type. We can just look at `train_raw`'s classes to get a sense of each columns data type.

```{r}
for (i in 1:ncol(train_raw)){
  cat(paste0(colnames(train_raw[i]),": ", class(train_raw[,i]), "\n"))
}
```

## Check for missing values

This section defines and uses a function to check for any missing values (NAs) in the datasets. A clean dataset with no missing values is crucial for building reliable models.

```{r}
check_NAs(train_df_raw = train_raw, test_df_raw = test_raw, target_col = "popularity")
```

Our check confirms that there are no NA values in any of the columns in the train and test datasets. In subsequent notebooks, we'll explore if there are non-sensical values in any of these columns but for now we can move on. 

## Unique values

In this final validation step, we examine the number of distinct values per column to identify which variables are good candidates for being treated as categorical or binary in our feature engineering.

```{r}
check_distinct_vals(train_df_raw = train_raw, test_df_raw = test_raw, target_col = "popularity")
```

The above table lists all of the distinct values for all of the columns. Since we are only interested in potential binary or categorical variables, let's filter the table down so it only displays the columns with less than 15 distinct values in either the train or test dataset.

The following table filters the full unique values summary to only show columns with fewer than 15 distinct values in either the train or test sets. This helps us quickly pinpoint potential categorical features.

```{r}
distinct_vals <- check_distinct_vals(train_df_raw = train_raw, test_df_raw = test_raw, target_col = "popularity", print_output = FALSE)

distinct_vals %>%
  dplyr::filter(Train_Distinct_Values < 15 | Test_Distict_Values < 15)
```

After filtering, we can see that:

  1) The columns *almost* match between train and test sets. It looks like there is one less time_signature in the test set.
  2) There are 5 columns that are either binary or categorical. Specifically:
    
  - `explicit` is a binary variable and can be made into a dummy column
  - `key` is a categorical variable and should be turned into factors
  - `mode` is a binary variable and can be made into a dummy column
  - `time_signature` is a categorical variable and should be turned into factors
  - `track_genre` is a categorical variable and should be turned into factors
    
## Conclusion

Based on our initial data validation, we can draw the following conclusions that will inform our next steps in the project:

  - *Data Integrity*: The train and test datasets are structurally sound, with consistent column types and no missing values across all features.
  - *Categorical Features*: We have identified five columns with a low number of unique values that are suitable for conversion to categorical factors: explicit, key, mode, time_signature, and track_genre.
  - *Target Column*: The popularity column is correctly identified as the target variable, present only in the training set.
  - *Structural Discrepancy*: The time_signature variable has a different number of distinct values between the training and testing sets. This will need to be addressed during the feature engineering phase to prevent errors in subsequent analysis.
  - *Unused Columns*: The id, album_name, and track_name columns are not relevant for modeling and should be removed from the datasets before any further analysis.

These findings provide a solid foundation for the data cleaning and preprocessing steps that will be carried out in the next notebook.
